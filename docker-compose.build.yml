version: "3"

services:
  stream_pose_ml_api:
    image: mrilikecoding/stream_pose_ml_api:latest
    ports:
      - "5001:5001"
    networks:
      - app-tier
    volumes:
      - ${PWD}/stream_pose_ml:/usr/src/app
      - ${PWD}/data/trained_models:/usr/src/app/data/trained_models
      - shared_models:/usr/src/app/tmp # Mount the named volume at /usr/src/app/tmp
    environment:
      - FLASK_APP=run.py
      - FLASK_ENV=development
      - PYTHONUNBUFFERED=1
    command: >
      bash -c "python -u run.py"

  web_ui:
    image: mrilikecoding/stream_pose_ml_web_ui:latest
    depends_on:
      - stream_pose_ml_api
    ports:
      - "3000:3000"
    stdin_open: true
    networks:
      - app-tier
    entrypoint: >
      bash -c "yarn build  && yarn serve --host --port 3000"
    volumes:
      - ./web_ui:/usr/src/app/my-app
      - ./web_ui/logs:/root/.npm/_logs
      - /usr/src/app/my-app/node_modules # anonymous persistent volume to prevent local machine override

  mlflow:
    image: mrilikecoding/stream_pose_ml_mlflow:latest
    networks:
      - app-tier
    ports:
      - "5002:5002" # Expose Flask API
      - "1234:1234" # Expose MLFlow Model Server API Subprocess
    environment:
      - FLASK_ENV=development
      - PYTHONUNBUFFERED=1
    volumes:
      - ./mlflow/app.py:/app.py # Mount the app.py file
      - shared_models:/models # Mount the named volume at /models

networks:
  app-tier:
    driver: bridge

volumes:
  shared_models:
