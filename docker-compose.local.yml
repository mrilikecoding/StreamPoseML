services:
  stream_pose_ml_api:
    build:
      context: .
      dockerfile: api/Dockerfile
      args:
        - env=dev  # Use dev environment for local development
        - flask_debug=True
    ports:
      - "5001:5001"
    networks:
      - app-tier
    volumes:
      - ${PWD}/api:/usr/src/app/api
      - ${PWD}/stream_pose_ml:/usr/src/app/stream_pose_ml  # Mount local package for development
      - ${PWD}/data/trained_models:/usr/src/app/data/trained_models
      - shared_models:/usr/src/app/tmp
    environment:
      - FLASK_APP=run.py
      - FLASK_ENV=development
      - PYTHONUNBUFFERED=1
    command: >
      bash -c "python -u run.py"

  web_ui:
    build:
      context: ./web_ui
    depends_on:
      - stream_pose_ml_api
    ports:
      - "3000:3000"
    stdin_open: true
    networks:
      - app-tier
    entrypoint: >
      bash -c "yarn build && yarn serve --host --port 3000"
    volumes:
      - ./web_ui:/usr/src/app/my-app
      - ./web_ui/logs:/root/.npm/_logs
      - /usr/src/app/my-app/node_modules # anonymous persistent volume to prevent local machine override

  mlflow:
    build:
      context: ./mlflow
    networks:
      - app-tier
    ports:
      - "5002:5002" # Expose Flask API
      - "1234:1234" # Expose MLFlow Model Server API Subprocess
    environment:
      - FLASK_ENV=development
      - PYTHONUNBUFFERED=1
    volumes:
      - ./mlflow/app.py:/app.py # Mount the app.py file
      - shared_models:/models # Mount the named volume at /models

networks:
  app-tier:
    driver: bridge

volumes:
  shared_models:
